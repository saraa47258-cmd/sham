#!/usr/bin/env python3
"""
Ø³ÙŠØ±ÙØ± Ø§Ù„Ù…Ø·Ø¹Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù† - ÙŠØ¯Ø¹Ù… Ø¹Ø¯Ø¯ ÙƒØ¨ÙŠØ± Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†ÙŠÙ†
Ù…Ø¹ Thread Pool Ùˆ Connection Pooling Ùˆ Rate Limiting
"""

import json
import os
import sys
import subprocess
import threading
import time
import hashlib
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import urlparse, parse_qs
from socketserver import ThreadingMixIn
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
import gzip

DATA_FILE = 'restaurant_data.json'
PROJECT_DIR = os.path.dirname(os.path.abspath(__file__))

# ==========================================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
# ==========================================
MAX_WORKERS = 100  # Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ø§Ù„ Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†ÙŠÙ† (Ù…Ø¶Ø§Ø¹Ù)
CACHE_TTL = 30  # Ø«ÙˆØ§Ù†ÙŠ Ù„Ù„ÙƒØ§Ø´ (Ø²ÙŠØ§Ø¯Ø© Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±)
RATE_LIMIT_REQUESTS = 200  # Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø© (Ù…Ø¶Ø§Ø¹Ù)
RATE_LIMIT_WINDOW = 60  # Ù†Ø§ÙØ°Ø© Ø§Ù„ÙˆÙ‚Øª Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
MAX_BODY_SIZE = 2 * 1024 * 1024  # 2MB Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
CONNECTION_TIMEOUT = 30  # Ù…Ù‡Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
KEEP_ALIVE = True  # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø§ØªØµØ§Ù„ Ù…ÙØªÙˆØ­
GZIP_MIN_SIZE = 1024  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø¶ØºØ· (1KB)

# ==========================================
# Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ù…Ø­Ø³Ù‘Ù†
# ==========================================
class CacheManager:
    def __init__(self, ttl=CACHE_TTL, max_size=1000):
        self.cache = {}
        self.ttl = ttl
        self.max_size = max_size
        self.lock = threading.RLock()  # RLock Ù„Ù„Ø³Ù…Ø§Ø­ Ø¨Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„
        self.hits = 0
        self.misses = 0
        self.last_cleanup = time.time()
        self.cleanup_interval = 60  # ØªÙ†Ø¸ÙŠÙ ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚Ø©
    
    def get(self, key):
        with self.lock:
            self._auto_cleanup()
            if key in self.cache:
                data, timestamp, access_count = self.cache[key]
                if time.time() - timestamp < self.ttl:
                    self.cache[key] = (data, timestamp, access_count + 1)
                    self.hits += 1
                    return data
                del self.cache[key]
            self.misses += 1
        return None
    
    def set(self, key, value, custom_ttl=None):
        with self.lock:
            # Ø¥Ø°Ø§ Ø§Ù…ØªÙ„Ø£ Ø§Ù„ÙƒØ§Ø´ØŒ Ù†Ø­Ø°Ù Ø§Ù„Ø£Ù‚Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹
            if len(self.cache) >= self.max_size:
                self._evict_lru()
            self.cache[key] = (value, time.time(), 1)
    
    def invalidate(self, pattern=None):
        with self.lock:
            if pattern:
                keys_to_delete = [k for k in self.cache if pattern in k]
                for k in keys_to_delete:
                    del self.cache[k]
            else:
                self.cache.clear()
    
    def _evict_lru(self):
        """Ø­Ø°Ù Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø£Ù‚Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹"""
        if not self.cache:
            return
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØµÙˆÙ„ ÙˆØ­Ø°Ù Ø§Ù„Ø£Ù‚Ù„
        sorted_keys = sorted(self.cache.keys(), key=lambda k: self.cache[k][2])
        keys_to_remove = sorted_keys[:len(sorted_keys) // 4]  # Ø­Ø°Ù 25%
        for k in keys_to_remove:
            del self.cache[k]
    
    def _auto_cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ù†ØªÙ‡ÙŠØ©"""
        now = time.time()
        if now - self.last_cleanup < self.cleanup_interval:
            return
        self.last_cleanup = now
        expired_keys = [k for k, (_, ts, _) in self.cache.items() if now - ts >= self.ttl]
        for k in expired_keys:
            del self.cache[k]
    
    def get_stats(self):
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙƒØ§Ø´"""
        total = self.hits + self.misses
        hit_rate = (self.hits / total * 100) if total > 0 else 0
        return {
            'size': len(self.cache),
            'hits': self.hits,
            'misses': self.misses,
            'hit_rate': f'{hit_rate:.1f}%'
        }

cache = CacheManager()

# ==========================================
# Ù†Ø¸Ø§Ù… Rate Limiting
# ==========================================
class RateLimiter:
    def __init__(self, max_requests=RATE_LIMIT_REQUESTS, window=RATE_LIMIT_WINDOW):
        self.requests = {}
        self.max_requests = max_requests
        self.window = window
        self.lock = threading.Lock()
    
    def is_allowed(self, ip):
        with self.lock:
            now = time.time()
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            if ip in self.requests:
                self.requests[ip] = [t for t in self.requests[ip] if now - t < self.window]
            else:
                self.requests[ip] = []
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯
            if len(self.requests[ip]) >= self.max_requests:
                return False
            
            self.requests[ip].append(now)
            return True
    
    def get_remaining(self, ip):
        with self.lock:
            if ip not in self.requests:
                return self.max_requests
            now = time.time()
            valid_requests = [t for t in self.requests[ip] if now - t < self.window]
            return max(0, self.max_requests - len(valid_requests))

rate_limiter = RateLimiter()

# ==========================================
# Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Thread Safety
# ==========================================
data_lock = threading.Lock()

def load_data():
    cached = cache.get('data')
    if cached:
        return cached
    
    with data_lock:
        if os.path.exists(DATA_FILE):
            try:
                with open(DATA_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    cache.set('data', data)
                    return data
            except (json.JSONDecodeError, IOError) as e:
                print(f'âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}')
        
        default_data = {
            'orders': [],
            'tables': [{'id': i, 'status': 'available', 'currentOrder': None} for i in range(1, 11)]
        }
        cache.set('data', default_data)
        return default_data

def save_data(data):
    with data_lock:
        try:
            # ÙƒØªØ§Ø¨Ø© Ø¢Ù…Ù†Ø© - Ù…Ù„Ù Ù…Ø¤Ù‚Øª Ø£ÙˆÙ„Ø§Ù‹
            temp_file = DATA_FILE + '.tmp'
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            os.replace(temp_file, DATA_FILE)
            cache.set('data', data)
            cache.invalidate('orders')
            cache.invalidate('tables')
        except IOError as e:
            print(f'âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}')
            raise

# Ù…ØªØºÙŠØ± Ù„ØªØªØ¨Ø¹ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø´Ø±
deploy_status = {'running': False, 'output': '', 'success': None}
deploy_lock = threading.Lock()

def run_deploy(site_id=None, template_id=None):
    """ØªÙ†ÙÙŠØ° Ø£Ù…Ø± Ø§Ù„Ù†Ø´Ø± ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ© Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨"""
    global deploy_status
    with deploy_lock:
        deploy_status = {'running': True, 'output': 'Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù†Ø´Ø±...', 'success': None}
    
    try:
        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ø´Ø± Ø­Ø³Ø¨ Ø§Ù„Ù‚Ø§Ù„Ø¨
        public_dir = PROJECT_DIR
        if template_id and template_id != 'current':
            template_path = os.path.join(PROJECT_DIR, 'templates', template_id)
            if os.path.exists(template_path):
                public_dir = template_path
        
        # Ø¥Ù†Ø´Ø§Ø¡ firebase.json Ù…Ø¤Ù‚Øª Ù„Ù„Ù†Ø´Ø± Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‚Ø§Ù„Ø¨ Ù…Ø®ØªØ§Ø±
        temp_firebase_json = None
        temp_firebaserc = None
        if template_id and template_id != 'current' and public_dir != PROJECT_DIR:
            temp_firebase_json = os.path.join(public_dir, 'firebase.json')
            temp_firebaserc = os.path.join(public_dir, '.firebaserc')
            
            # Ø¥Ù†Ø´Ø§Ø¡ firebase.json Ù…Ø¹ target ØµØ­ÙŠØ­
            target_name = site_id if site_id else 'default-site'
            firebase_config = {
                "hosting": {
                    "target": target_name,
                    "public": ".",
                    "ignore": ["firebase.json", ".firebaserc", "**/.*", "**/node_modules/**"],
                    "rewrites": [{"source": "**", "destination": "/index.html"}]
                }
            }
            with open(temp_firebase_json, 'w', encoding='utf-8') as f:
                json.dump(firebase_config, f, ensure_ascii=False, indent=2)
            
            # Ø¥Ù†Ø´Ø§Ø¡ .firebaserc Ù„Ø±Ø¨Ø· Ø§Ù„Ù‡Ø¯Ù Ø¨Ø§Ù„Ù…ÙˆÙ‚Ø¹
            firebaserc_config = {
                "projects": {
                    "default": "restaurant-system-demo"
                },
                "targets": {
                    "restaurant-system-demo": {
                        "hosting": {
                            target_name: [site_id if site_id else "restaurant-system-demo"]
                        }
                    }
                }
            }
            with open(temp_firebaserc, 'w', encoding='utf-8') as f:
                json.dump(firebaserc_config, f, ensure_ascii=False, indent=2)
            
            # Ù†Ø³Ø® Ù…Ù„ÙØ§Øª JS Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù„Ù„Ù‚Ø§Ù„Ø¨
            js_src = os.path.join(PROJECT_DIR, 'js')
            js_dest = os.path.join(public_dir, 'js')
            css_src = os.path.join(PROJECT_DIR, 'css')
            css_dest = os.path.join(public_dir, 'css')
            
            import shutil
            if os.path.exists(js_src) and not os.path.exists(js_dest):
                shutil.copytree(js_src, js_dest)
            if os.path.exists(css_src) and not os.path.exists(css_dest):
                shutil.copytree(css_src, css_dest)
            
            # Ù†Ø³Ø® Ù…Ù„ÙØ§Øª HTML Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            essential_files = ['login-restaurant.html', 'admin.html', 'cashier.html', 
                              'waiter.html', 'menu.html', 'store.html', 'store-admin.html',
                              'inventory.html', 'profile.html']
            for ef in essential_files:
                src_file = os.path.join(PROJECT_DIR, ef)
                dest_file = os.path.join(public_dir, ef)
                if os.path.exists(src_file) and not os.path.exists(dest_file):
                    shutil.copy2(src_file, dest_file)
        
        # Ø¨Ù†Ø§Ø¡ Ø£Ù…Ø± Ø§Ù„Ù†Ø´Ø±
        if site_id:
            # Ù†Ø´Ø± Ø¥Ù„Ù‰ Ù…ÙˆÙ‚Ø¹ Ù…Ø­Ø¯Ø¯
            cmd = f'firebase deploy --only hosting:{site_id} --project restaurant-system-demo'
        else:
            cmd = 'firebase deploy --only hosting --project restaurant-system-demo'
        
        result = subprocess.run(
            cmd,
            shell=True,
            cwd=public_dir,
            capture_output=True,
            text=True,
            timeout=180
        )
        
        output = result.stdout + result.stderr
        success = result.returncode == 0
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        if temp_firebase_json and os.path.exists(temp_firebase_json):
            os.remove(temp_firebase_json)
        if temp_firebaserc and os.path.exists(temp_firebaserc):
            os.remove(temp_firebaserc)
        
        with deploy_lock:
            deploy_status = {
                'running': False,
                'output': output,
                'success': success,
                'siteId': site_id,
                'templateId': template_id
            }
        
    except subprocess.TimeoutExpired:
        with deploy_lock:
            deploy_status = {
                'running': False,
                'output': 'Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ù†Ø´Ø± (Ø£ÙƒØ«Ø± Ù…Ù† 3 Ø¯Ù‚Ø§Ø¦Ù‚)',
                'success': False
            }
    except Exception as e:
        with deploy_lock:
            deploy_status = {
                'running': False,
                'output': f'Ø®Ø·Ø£: {str(e)}',
                'success': False
            }


def list_hosting_sites(project_id=None):
    """Ø¥Ø±Ø¬Ø§Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ù…ÙˆØ§Ù‚Ø¹ Firebase Hosting Ø¹Ø¨Ø± firebase-tools.

    Ù…Ù„Ø§Ø­Ø¸Ø©: ÙÙŠ Ø­Ø§Ù„ Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù…Ø´Ø±ÙˆØ¹ Ù†Ø´Ø· ÙÙŠ Firebase CLIØŒ Ù…Ø±Ù‘Ø± project_id.
    """
    cache_key = f"hosting_sites:{project_id or 'default'}"
    # ÙƒØ§Ø´ Ø¨Ø³ÙŠØ· Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ CLI
    cached = cache.get(cache_key)
    if cached is not None:
        return cached

    # Ù…Ø­Ø§ÙˆÙ„Ø© JSON Ø£ÙˆÙ„Ø§Ù‹
    if project_id:
        cmd_candidates = [
            f'firebase hosting:sites:list --project {project_id} --json',
            f'firebase hosting:sites:list --project {project_id}'
        ]
    else:
        cmd_candidates = [
            'firebase hosting:sites:list --json',
            'firebase hosting:sites:list'
        ]

    last_error = None
    for cmd in cmd_candidates:
        try:
            result = subprocess.run(
                cmd,
                shell=True,
                cwd=PROJECT_DIR,
                capture_output=True,
                text=True,
                timeout=30
            )

            stdout = (result.stdout or '').strip()
            stderr = (result.stderr or '').strip()
            combined = (stdout + '\n' + stderr).strip()

            if result.returncode != 0:
                last_error = combined or f'ÙØ´Ù„ ØªÙ†ÙÙŠØ° Ø§Ù„Ø£Ù…Ø±: {cmd}'
                # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§ØªØŒ Ù„Ø§ Ù†Ø­Ø§ÙˆÙ„ Ø§Ù„Ù…Ø²ÙŠØ¯
                if 'Permission' in combined or '403' in combined or 'denied' in combined.lower():
                    cache.set(cache_key, [])
                    raise RuntimeError('Ù„Ø§ ØªÙˆØ¬Ø¯ ØµÙ„Ø§Ø­ÙŠØ§Øª Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Firebase Hosting. ÙŠØ±Ø¬Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Firebase CLI Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…: firebase login')
                continue

            sites = []

            if '--json' in cmd:
                try:
                    payload = json.loads(stdout)
                    # Ø¨Ù†ÙŠØ© firebase-tools Ù‚Ø¯ ØªØ®ØªÙ„ÙØ› Ù†Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ IDs Ø¨Ø´ÙƒÙ„ Ù…Ø±Ù†
                    candidates = []
                    if isinstance(payload, dict):
                        for key in ['result', 'results', 'data']:
                            if key in payload:
                                candidates = payload.get(key)
                                break
                        if not candidates and 'hosting' in payload:
                            candidates = payload.get('hosting')
                    if isinstance(candidates, dict) and 'sites' in candidates:
                        candidates = candidates.get('sites')
                    if isinstance(candidates, list):
                        for item in candidates:
                            if isinstance(item, str):
                                sites.append(item)
                            elif isinstance(item, dict):
                                for k in ['site', 'siteId', 'name', 'id']:
                                    if k in item and isinstance(item[k], str):
                                        sites.append(item[k])
                                        break
                except Exception:
                    # fallback parse Ø§Ù„Ù†Øµ
                    pass

            if not sites:
                # Parse Ø§Ù„Ù†Øµ: Ø§Ù„ØªÙ‚Ø· ÙƒÙ„Ù…Ø§Øª Ø´Ø¨ÙŠÙ‡Ø© Ø¨Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ (a-z0-9-)
                import re
                # ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙ‚Ø§Ø· Ø±ÙˆØ§Ø¨Ø· web.app Ø£Ùˆ firebaseapp.com
                for line in combined.splitlines():
                    line = line.strip()
                    if not line:
                        continue
                    # Ù‚Ø¯ ÙŠØ¸Ù‡Ø± Ø¬Ø¯ÙˆÙ„Ø› Ù†Ø£Ø®Ø° Ø£ÙˆÙ„ Ø¹Ù…ÙˆØ¯ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ù†Ø§Ø³Ø¨Ø§Ù‹
                    first = re.split(r'\s+', line)[0]
                    if re.fullmatch(r'[a-z0-9-]{3,}', first) and not first.endswith('web'):
                        sites.append(first)

            # ØªÙ†Ø¸ÙŠÙ duplicates
            cleaned = []
            seen = set()
            for s in sites:
                s = (s or '').strip()
                if not s or s in seen:
                    continue
                seen.add(s)
                cleaned.append(s)

            cache.set(cache_key, cleaned)
            return cleaned

        except Exception as e:
            last_error = str(e)
            continue

    # ÙØ´Ù„ ÙƒÙ„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª
    cache.set(cache_key, [])
    error_msg = last_error or 'ØªØ¹Ø°Ø± Ø¬Ù„Ø¨ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø§Ø³ØªØ¶Ø§ÙØ©'
    # ØªØ­Ø³ÙŠÙ† Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
    if 'Permission' in error_msg or '403' in error_msg or 'denied' in error_msg.lower():
        error_msg = 'Ù„Ø§ ØªÙˆØ¬Ø¯ ØµÙ„Ø§Ø­ÙŠØ§Øª Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Firebase Hosting. ÙŠØ±Ø¬Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Firebase CLI.'
    raise RuntimeError(error_msg)

class RestaurantHandler(BaseHTTPRequestHandler):
    # ØªØ¹Ø·ÙŠÙ„ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
    def log_message(self, format, *args):
        pass  # ÙŠÙ…ÙƒÙ† ØªÙØ¹ÙŠÙ„Ù‡Ø§ Ù„Ù„ØªØµØ­ÙŠØ­
    
    def get_client_ip(self):
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ IP Ø§Ù„Ø¹Ù…ÙŠÙ„ (ÙŠØ¯Ø¹Ù… proxy)
        forwarded = self.headers.get('X-Forwarded-For')
        if forwarded:
            return forwarded.split(',')[0].strip()
        return self.client_address[0]
    
    def check_rate_limit(self):
        ip = self.get_client_ip()
        if not rate_limiter.is_allowed(ip):
            self.send_error_json(429, 'ØªÙ… ØªØ¬Ø§ÙˆØ² Ø­Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø©')
            return False
        return True
    
    def send_error_json(self, code, message):
        try:
            self.send_response(code)
            self.send_header('Content-Type', 'application/json; charset=utf-8')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            self.wfile.write(json.dumps({'error': message}, ensure_ascii=False).encode('utf-8'))
            self.wfile.flush()
        except (ConnectionAbortedError, BrokenPipeError, OSError):
            pass  # Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù†Ù‚Ø·Ø¹
        except Exception as e:
            print(f'âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ error JSON: {e}')
    
    def do_GET(self):
        if not self.check_rate_limit():
            return
        
        parsed = urlparse(self.path)
        
        if parsed.path == '/api/orders':
            cached = cache.get('orders')
            if cached:
                self.send_json(cached)
            else:
                orders = load_data()['orders']
                cache.set('orders', orders)
                self.send_json(orders)
        elif parsed.path == '/api/tables':
            cached = cache.get('tables')
            if cached:
                self.send_json(cached)
            else:
                tables = load_data()['tables']
                cache.set('tables', tables)
                self.send_json(tables)
        elif parsed.path == '/api/data':
            self.send_json(load_data())
        elif parsed.path == '/api/deploy/status':
            with deploy_lock:
                self.send_json(deploy_status)
        elif parsed.path == '/api/hosting/sites':
            try:
                qs = parse_qs(parsed.query)
                project_id = (qs.get('project') or [None])[0]
                sites = list_hosting_sites(project_id=project_id)
                self.send_json({'success': True, 'sites': sites})
            except RuntimeError as e:
                # Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª Ø£Ùˆ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Firebase
                error_msg = str(e)
                if 'Permission' in error_msg or '403' in error_msg:
                    error_msg = 'Ù„Ø§ ØªÙˆØ¬Ø¯ ØµÙ„Ø§Ø­ÙŠØ§Øª Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Firebase Hosting. ÙŠØ±Ø¬Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Firebase CLI.'
                self.send_json({'success': False, 'error': error_msg, 'sites': []})
            except Exception as e:
                # Ø®Ø·Ø£ Ø¹Ø§Ù…
                error_msg = f'Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹: {str(e)}'
                print(f'âš ï¸ {error_msg}')
                try:
                    self.send_json({'success': False, 'error': error_msg, 'sites': []})
                except:
                    pass  # Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù†Ù‚Ø·Ø¹
        elif parsed.path == '/api/health':
            # Ù†Ù‚Ø·Ø© ÙØ­Øµ Ø§Ù„ØµØ­Ø© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
            health_data = {
                'status': 'healthy',
                'timestamp': time.time(),
                'uptime': time.time() - server_start_time if 'server_start_time' in globals() else 0,
                'rate_limit_remaining': rate_limiter.get_remaining(self.get_client_ip()),
                'cache_stats': cache.get_stats(),
                'connections': {
                    'max_workers': MAX_WORKERS,
                    'rate_limit': f'{RATE_LIMIT_REQUESTS}/{RATE_LIMIT_WINDOW}s'
                }
            }
            self.send_json(health_data)
        elif parsed.path == '/api/stats':
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
            self.send_json({
                'cache': cache.get_stats(),
                'settings': {
                    'max_workers': MAX_WORKERS,
                    'cache_ttl': CACHE_TTL,
                    'rate_limit': RATE_LIMIT_REQUESTS,
                    'max_body_size': MAX_BODY_SIZE
                }
            })
        elif parsed.path == '/api/templates':
            # Ø¬Ù„Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ù…ØªØ§Ø­Ø© Ù…Ø¹ Ø§Ù„Ø£Ù‚Ø³Ø§Ù…
            try:
                templates_dir = os.path.join(PROJECT_DIR, 'templates')
                templates_file = os.path.join(templates_dir, 'templates.json')
                categories_file = os.path.join(templates_dir, 'categories.json')

                # 1) ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨/Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù‚Ø¯ÙŠÙ… Ø¥Ù† ÙˆØ¬Ø¯ (ØªÙˆØ§ÙÙ‚ Ø®Ù„ÙÙŠ)
                templates_data = {}
                if os.path.exists(templates_file):
                    with open(templates_file, 'r', encoding='utf-8') as f:
                        templates_data = json.load(f) or {}

                templates_list = list(templates_data.get('templates', []) or [])

                # 2) ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ù…Ù† Ù…Ù„Ù Ù…Ø³ØªÙ‚Ù„ Ø¥Ù† ÙˆØ¬Ø¯ (Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù‡)
                categories_list = list(templates_data.get('categories', []) or [])
                if os.path.exists(categories_file):
                    try:
                        with open(categories_file, 'r', encoding='utf-8') as f:
                            loaded_categories = json.load(f)
                        if isinstance(loaded_categories, list):
                            categories_list = loaded_categories
                    except Exception:
                        # Ù„Ùˆ ÙØ´Ù„Ù†Ø§ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© categories.json Ù†ÙØ¨Ù‚ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø¯ÙŠÙ…
                        pass

                # 3) Ø¯Ù…Ø¬/Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ù…Ù† Ù…Ù„ÙØ§Øª template.json Ø¯Ø§Ø®Ù„ ÙƒÙ„ Ù‚Ø§Ù„Ø¨
                discovered = {}
                if os.path.isdir(templates_dir):
                    for entry in os.listdir(templates_dir):
                        entry_path = os.path.join(templates_dir, entry)
                        if not os.path.isdir(entry_path):
                            continue
                        template_json = os.path.join(entry_path, 'template.json')
                        if not os.path.exists(template_json):
                            continue
                        try:
                            with open(template_json, 'r', encoding='utf-8') as f:
                                tpl = json.load(f)
                            if isinstance(tpl, dict) and tpl.get('id'):
                                discovered[str(tpl['id'])] = tpl
                        except Exception:
                            continue

                # override existing templates by id, and append new ones
                merged = []
                seen_ids = set()
                for tpl in templates_list:
                    tpl_id = None
                    if isinstance(tpl, dict):
                        tpl_id = tpl.get('id')
                    if tpl_id and str(tpl_id) in discovered:
                        merged.append(discovered[str(tpl_id)])
                        seen_ids.add(str(tpl_id))
                    else:
                        merged.append(tpl)
                        if tpl_id:
                            seen_ids.add(str(tpl_id))

                for tpl_id, tpl in discovered.items():
                    if tpl_id not in seen_ids:
                        merged.append(tpl)
                        seen_ids.add(tpl_id)

                self.send_json({
                    'success': True,
                    'templates': merged,
                    'categories': categories_list
                })
            except Exception as e:
                self.send_json({'success': False, 'error': str(e), 'templates': [], 'categories': []})
        else:
            # Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ©
            self.serve_static_file()
    
    def serve_static_file(self):
        """Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© Ù…Ø¹ ÙƒØ§Ø´"""
        parsed = urlparse(self.path)
        path = parsed.path
        
        if path == '/':
            # Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¹Ù†Ø¯ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ±
            # Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: ÙØªØ­ ØµÙØ­Ø© Ø§Ù„Ø³ÙˆØ¨Ø± Ø£Ø¯Ù…Ù† Ø£ÙˆÙ„Ø§Ù‹ Ø¨Ø¯Ù„ ØµÙØ­Ø© Ø§Ù„Ù‚Ø§Ù„Ø¨/Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¹Ø§Ù…Ø©
            preferred = '/super-admin.html'
            preferred_path = os.path.join(PROJECT_DIR, preferred.lstrip('/'))
            path = preferred if os.path.exists(preferred_path) else '/index.html'
        
        file_path = os.path.join(PROJECT_DIR, path.lstrip('/'))
        
        if not os.path.exists(file_path) or not os.path.isfile(file_path):
            self.send_error(404)
            return
        
        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
        content_type = self.guess_content_type(file_path)
        
        try:
            with open(file_path, 'rb') as f:
                content = f.read()
            
            self.send_response(200)
            self.send_header('Content-Type', content_type)
            self.send_header('Access-Control-Allow-Origin', '*')
            
            # Cache headers Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ©
            if any(file_path.endswith(ext) for ext in ['.css', '.js', '.png', '.jpg', '.ico']):
                self.send_header('Cache-Control', 'public, max-age=3600')
            else:
                self.send_header('Cache-Control', 'no-cache')
            
            # Ø¶ØºØ· Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ©
            if content_type.startswith('text/') or content_type == 'application/javascript':
                accept_encoding = self.headers.get('Accept-Encoding', '')
                if 'gzip' in accept_encoding:
                    content = gzip.compress(content)
                    self.send_header('Content-Encoding', 'gzip')
            
            self.send_header('Content-Length', len(content))
            self.end_headers()
            self.wfile.write(content)
            self.wfile.flush()
        except (ConnectionAbortedError, BrokenPipeError, OSError):
            pass  # Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù†Ù‚Ø·Ø¹
        except IOError:
            try:
                self.send_error(500)
            except:
                pass  # Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù†Ù‚Ø·Ø¹
    
    def guess_content_type(self, path):
        ext = os.path.splitext(path)[1].lower()
        types = {
            '.html': 'text/html; charset=utf-8',
            '.css': 'text/css; charset=utf-8',
            '.js': 'application/javascript; charset=utf-8',
            '.json': 'application/json; charset=utf-8',
            '.png': 'image/png',
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.gif': 'image/gif',
            '.ico': 'image/x-icon',
            '.svg': 'image/svg+xml',
            '.woff': 'font/woff',
            '.woff2': 'font/woff2',
        }
        return types.get(ext, 'application/octet-stream')
    
    def do_POST(self):
        if not self.check_rate_limit():
            return
        
        parsed = urlparse(self.path)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        content_length = int(self.headers.get('Content-Length', 0))
        if content_length > MAX_BODY_SIZE:
            self.send_error_json(413, 'Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹')
            return
        
        try:
            post_data = self.rfile.read(content_length)
            body = json.loads(post_data.decode('utf-8'))
        except (json.JSONDecodeError, UnicodeDecodeError) as e:
            self.send_error_json(400, 'Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ØµØ§Ù„Ø­Ø©')
            return
        
        data = load_data()
        
        if parsed.path == '/api/orders':
            # Ø¥Ø¶Ø§ÙØ© Ø·Ù„Ø¨ Ø¬Ø¯ÙŠØ¯
            order = body
            order['id'] = int(order.get('id', 0)) or int(__import__('time').time() * 1000)
            data['orders'].insert(0, order)
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø·Ø§ÙˆÙ„Ø©
            table_id = order.get('tableId')
            for table in data['tables']:
                if table['id'] == table_id:
                    table['status'] = 'pending'
                    table['currentOrder'] = order['id']
                    break
            
            save_data(data)
            self.send_json({'success': True, 'order': order})
        
        elif parsed.path == '/api/orders/update':
            # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ø·Ù„Ø¨
            order_id = body.get('id')
            new_status = body.get('status')
            
            for order in data['orders']:
                if order['id'] == order_id:
                    order['status'] = new_status
                    
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø·Ø§ÙˆÙ„Ø©
                    for table in data['tables']:
                        if table['id'] == order['tableId']:
                            if new_status == 'completed':
                                table['status'] = 'available'
                                table['currentOrder'] = None
                            elif new_status == 'preparing':
                                table['status'] = 'occupied'
                            break
                    break
            
            save_data(data)
            self.send_json({'success': True})
        
        elif parsed.path == '/api/tables/update':
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø·Ø§ÙˆÙ„Ø©
            table_id = body.get('id')
            updates = body.get('updates', {})
            
            for table in data['tables']:
                if table['id'] == table_id:
                    table.update(updates)
                    break
            
            save_data(data)
            self.send_json({'success': True})
        
        elif parsed.path == '/api/tables/count':
            # ØªØºÙŠÙŠØ± Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø§ÙˆÙ„Ø§Øª
            count = body.get('count', 10)
            current = data['tables']
            new_tables = []
            for i in range(1, count + 1):
                existing = next((t for t in current if t['id'] == i), None)
                if existing:
                    new_tables.append(existing)
                else:
                    new_tables.append({'id': i, 'status': 'available', 'currentOrder': None})
            data['tables'] = new_tables
            save_data(data)
            self.send_json({'success': True})
        
        elif parsed.path == '/api/orders/delete':
            order_id = body.get('id')
            data['orders'] = [o for o in data['orders'] if o['id'] != order_id]
            save_data(data)
            self.send_json({'success': True})
        
        elif parsed.path == '/api/deploy':
            # Ø¨Ø¯Ø¡ Ø§Ù„Ù†Ø´Ø± Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨
            site_id = body.get('siteId')
            template_id = body.get('templateId', 'current')
            
            with deploy_lock:
                if deploy_status['running']:
                    self.send_json({'success': False, 'error': 'Ø§Ù„Ù†Ø´Ø± Ù‚ÙŠØ¯ Ø§Ù„ØªÙ†ÙÙŠØ° Ø­Ø§Ù„ÙŠØ§Ù‹'})
                    return
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø´Ø± ÙÙŠ Ø®ÙŠØ· Ù…Ù†ÙØµÙ„
            thread = threading.Thread(target=run_deploy, args=(site_id, template_id), daemon=True)
            thread.start()
            self.send_json({'success': True, 'message': 'Ø¨Ø¯Ø£ Ø§Ù„Ù†Ø´Ø±', 'siteId': site_id, 'templateId': template_id})
        
        else:
            self.send_error(404)
    
    def send_json(self, obj, compress=False):
        try:
            content = json.dumps(obj, ensure_ascii=False).encode('utf-8')
            
            self.send_response(200)
            self.send_header('Content-Type', 'application/json; charset=utf-8')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
            self.send_header('Access-Control-Allow-Headers', 'Content-Type')
            
            # Ø¶ØºØ· Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
            accept_encoding = self.headers.get('Accept-Encoding', '')
            if compress and 'gzip' in accept_encoding and len(content) > 1024:
                content = gzip.compress(content)
                self.send_header('Content-Encoding', 'gzip')
            
            self.send_header('Content-Length', len(content))
            self.end_headers()
            self.wfile.write(content)
            self.wfile.flush()
        except (ConnectionAbortedError, BrokenPipeError, OSError) as e:
            # Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù†Ù‚Ø·Ø¹ - Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            pass
        except Exception as e:
            # Ø®Ø·Ø£ Ø¢Ø®Ø± - ØªØ³Ø¬ÙŠÙ„ ÙÙ‚Ø·
            print(f'âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ JSON: {e}')
    
    def do_OPTIONS(self):
        self.send_response(200)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.send_header('Access-Control-Max-Age', '86400')  # 24 Ø³Ø§Ø¹Ø©
        self.end_headers()


# ==========================================
# Ø³ÙŠØ±ÙØ± Ù…Ø¹ Ø¯Ø¹Ù… Threading
# ==========================================
class ThreadedHTTPServer(ThreadingMixIn, HTTPServer):
    """Ø³ÙŠØ±ÙØ± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø®ÙŠÙˆØ· Ù„Ø¯Ø¹Ù… Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©"""
    daemon_threads = True
    request_queue_size = 100
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)


if __name__ == '__main__':
    # Ø¶Ù…Ø§Ù† Ø£Ù† Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ØªØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ù„Ù‰ Windows (ØªØ¬Ù†Ø¨ UnicodeEncodeError)
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except Exception:
        pass

    port = 3000
    host = 'localhost'  # Ø§Ø³ØªØ®Ø¯Ø§Ù… localhost Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 0.0.0.0
    server_start_time = time.time()  # ØªØªØ¨Ø¹ ÙˆÙ‚Øª Ø§Ù„Ø¨Ø¯Ø¡
    server = ThreadedHTTPServer((host, port), RestaurantHandler)

    banner = f'''
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     ğŸ½ï¸  Ø³ÙŠØ±ÙØ± Ø§Ù„Ù…Ø·Ø¹Ù… Ø§Ù„Ù…Ø­Ø³Ù‘Ù† v2.0 - Ø£Ø¯Ø§Ø¡ ÙØ§Ø¦Ù‚               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸŒ Ø§Ù„Ø±Ø§Ø¨Ø·: http://localhost:{port}                            â•‘
â•‘  ğŸ“± Ù„Ù„Ù‡Ø§ØªÙ: http://192.168.1.112:{port}                        â•‘
â•‘  âš¡ Ø§Ù„Ø®ÙŠÙˆØ· Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©: {MAX_WORKERS}                                 â•‘
â•‘  ğŸ›¡ï¸  Rate Limit: {RATE_LIMIT_REQUESTS} Ø·Ù„Ø¨/{RATE_LIMIT_WINDOW} Ø«Ø§Ù†ÙŠØ©                       â•‘
â•‘  ğŸ“¦ Ø§Ù„ÙƒØ§Ø´: {CACHE_TTL} Ø«ÙˆØ§Ù†ÙŠ (LRU Ù…Ø¹ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª)                    â•‘
â•‘  ğŸ”§ API: /api/health | /api/stats                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    '''

    try:
        print(banner)
    except UnicodeEncodeError:
        # fallback Ø¨Ø³ÙŠØ· Ø¨Ø¯ÙˆÙ† Ø±Ù…ÙˆØ²/Ø¹Ø±Ø¨ÙŠ
        print(f"Server started on http://localhost:{port} (UTF-8 output not supported in this console)")
    
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        print('\nğŸ‘‹ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø³ÙŠØ±ÙØ±')
        print(f'ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {cache.get_stats()}')
        server.shutdown()
